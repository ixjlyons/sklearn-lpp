{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Preserving Projections: scikit-learn implementation\n",
    "\n",
    "[Locality preserving projection (LPP)](http://people.cs.uchicago.edu/~xiaofei/LPP.html) was introduced by Xiaofei He and Partha Niyogi at NIPS 2003. They provide a MATLAB implementation, but we'll be implementing it using scikit-learn, which provides some nice features that will do much of the work for us.\n",
    "\n",
    "LPP is a manifold learning technique for dimensionality reduction, useful when you have high-dimensional input that you suspect can be described well in only a few dimensions and want to work with it in a space of much lower dimensionality. The classic example, and the one implemented in the NIPS paper, is projecting images of faces to 2D space for visualization.\n",
    "\n",
    "The algorithm is based on spectral graph theory, and the main idea is to construct a graph in which like samples are connected with large weights, then project the samples to the low-dimensional space while maintaining the neighborhood structure. It is a linear transformation (there is a kernalized version, which allows for non-linear projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345)\n",
    "X = np.random.rand(100, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Construct the Adjacency Graph\n",
    "\n",
    "An adjacency graph simply describes which input samples are connected. Inputs that are similar should be connected (i.e. they are neighbors), and inputs that are dissimilar should not be connected. A simple way to represent the adjacency graph is an $m \\times m$ matrix, where $m$ is the number of samples you have. If input $\\mathbf{x}_i$ is connected to $\\mathbf{x}_j$, the entry of the matrix at row $i$ and column $j$ is $1$, otherwise it is $0$. The matrix is then symmetric (if $\\mathbf{x}_i$ is connected to $\\mathbf{x}_j$, then $\\mathbf{x}_j$ is connected to $\\mathbf{x}_i$) and it is likely sparse (depending on how neighbors are determined).\n",
    "\n",
    "He and Niyogi describe two methods of determining which samples are close to one another:\n",
    "\n",
    "* $\\epsilon$-neighborhoods: nodes $i$ and $j$ are connected if $||\\mathbf{x}_i - \\mathbf{x}_j||^2_2 < \\epsilon$\n",
    "* $k$ nearest neighbor: nodes $i$ and $j$ are connected if $i$ is one of $k$ nearest neighbors of $j$ or vice versa (neighbors determined by Euclidean distance)\n",
    "\n",
    "The authors also note that adjacency can be determined using other (less principled) methods and that LPP will attempt to preserve those neighborhoods.\n",
    "\n",
    "scikit-learn has a subpackage (`neighbors`) for creating graphs and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# generate a k nearest neighbors adjacency graph\n",
    "A = neighbors.kneighbors_graph(X, 4, include_self=True)\n",
    "\n",
    "# generate an epsilon-neighborhood graph\n",
    "A = neighbors.radius_neighbors_graph(X, 0.02, include_self=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
